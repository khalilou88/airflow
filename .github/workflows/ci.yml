name: Test Airflow Docker Compose

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  workflow_dispatch:

jobs:
  test-airflow-compose:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Check system resources
        run: |
          echo "Checking system resources..."
          echo "Memory available:"
          docker run --rm "debian:bookworm-slim" bash -c 'numfmt --to iec $(echo $(($(getconf _PHYS_PAGES) * $(getconf PAGE_SIZE))))'
          echo "Disk space:"
          df -h
          echo "CPU info:"
          nproc

      - name: Create required directories and environment
        run: |
          # Create directories as per official documentation
          mkdir -p ./dags ./logs ./plugins ./config
          # Set AIRFLOW_UID for Linux as per official docs
          echo "AIRFLOW_UID=$(id -u)" > .env
          # Verify the .env file
          cat .env

      - name: Create a simple test DAG
        run: |
          cat > ./dags/test_dag.py << 'EOF'
          from datetime import datetime, timedelta
          from airflow import DAG
          from airflow.operators.bash import BashOperator

          default_args = {
              'owner': 'test',
              'depends_on_past': False,
              'start_date': datetime(2024, 1, 1),
              'email_on_failure': False,
              'email_on_retry': False,
              'retries': 1,
              'retry_delay': timedelta(minutes=5),
          }

          dag = DAG(
              'test_dag',
              default_args=default_args,
              description='A simple test DAG',
              schedule_interval=timedelta(days=1),
              catchup=False,
              tags=['test'],
          )

          test_task = BashOperator(
              task_id='test_task',
              bash_command='echo "Hello from Airflow test!"',
              dag=dag,
          )
          EOF

      - name: Initialize Airflow database
        run: |
          # Initialize the database as per official documentation
          echo "Running airflow-init to initialize database and create admin user..."
          docker compose up airflow-init

      - name: Start Airflow services
        run: |
          # Start all Airflow services as per official documentation
          echo "Starting Airflow services..."
          docker compose up -d

      - name: Wait for services to be healthy
        run: |
          echo "Waiting for services to be healthy..."

          # Function to check if container is healthy
          check_health() {
            local service=$1
            local timeout=${2:-300}
            local count=0

            while [ $count -lt $timeout ]; do
              # First check if the service exists and is running
              if ! docker compose ps "$service" --format json >/dev/null 2>&1; then
                echo "‚ö†Ô∏è  Service $service not found or not running"
                sleep 5
                count=$((count + 5))
                echo "Waiting for $service... ($count/$timeout seconds)"
                continue
              fi

              # Get the JSON output and debug it
              local json_output
              json_output=$(docker compose ps "$service" --format json 2>/dev/null)
              
              if [ -z "$json_output" ]; then
                echo "‚ö†Ô∏è  No output from docker compose ps for $service"
                sleep 5
                count=$((count + 5))
                echo "Waiting for $service... ($count/$timeout seconds)"
                continue
              fi

              # Check if it's an array or single object
              local health_status
              if echo "$json_output" | jq -e '. | type' | grep -q "array"; then
                # It's an array, use index 0
                health_status=$(echo "$json_output" | jq -r '.[0].Health // empty' 2>/dev/null)
              else
                # It's a single object
                health_status=$(echo "$json_output" | jq -r '.Health // empty' 2>/dev/null)
              fi

              # If Health field doesn't exist, check State instead
              if [ -z "$health_status" ]; then
                if echo "$json_output" | jq -e '. | type' | grep -q "array"; then
                  health_status=$(echo "$json_output" | jq -r '.[0].State // empty' 2>/dev/null)
                else
                  health_status=$(echo "$json_output" | jq -r '.State // empty' 2>/dev/null)
                fi
              fi

              # Check various healthy states
              if [[ "$health_status" == "healthy" ]] || [[ "$health_status" == "running" ]]; then
                echo "‚úÖ $service is healthy (status: $health_status)"
                return 0
              fi

              sleep 5
              count=$((count + 5))
              echo "Waiting for $service (status: $health_status)... ($count/$timeout seconds)"
            done

            echo "‚ùå $service failed to become healthy within $timeout seconds"
            return 1
          }

          # Alternative function using docker inspect (more reliable)
          check_health_inspect() {
            local service=$1
            local timeout=${2:-300}
            local count=0

            while [ $count -lt $timeout ]; do
              # Get container ID for the service
              local container_id
              container_id=$(docker compose ps -q "$service" 2>/dev/null)
              
              if [ -z "$container_id" ]; then
                echo "‚ö†Ô∏è  Service $service not found"
                sleep 5
                count=$((count + 5))
                echo "Waiting for $service... ($count/$timeout seconds)"
                continue
              fi

              # Check health using docker inspect
              local health_status
              health_status=$(docker inspect "$container_id" --format='{{.State.Health.Status}}' 2>/dev/null)
              
              # If no health check is defined, check if container is running
              if [ "$health_status" = "<no value>" ] || [ -z "$health_status" ]; then
                health_status=$(docker inspect "$container_id" --format='{{.State.Status}}' 2>/dev/null)
              fi

              if [[ "$health_status" == "healthy" ]] || [[ "$health_status" == "running" ]]; then
                echo "‚úÖ $service is healthy (status: $health_status)"
                return 0
              fi

              sleep 5
              count=$((count + 5))
              echo "Waiting for $service (status: $health_status)... ($count/$timeout seconds)"
            done

            echo "‚ùå $service failed to become healthy within $timeout seconds"
            return 1
          }

          # Function to debug docker compose output
          debug_service() {
            local service=$1
            echo "üîç Debugging $service:"
            echo "Raw docker compose ps output:"
            docker compose ps "$service" --format json | jq '.' 2>/dev/null || echo "Failed to parse JSON"
            echo "Container status via docker inspect:"
            local container_id
            container_id=$(docker compose ps -q "$service" 2>/dev/null)
            if [ -n "$container_id" ]; then
              docker inspect "$container_id" --format='Health: {{.State.Health.Status}}, Status: {{.State.Status}}' 2>/dev/null
            fi
            echo "---"
          }

          # Wait for core services to be healthy
          echo "Checking core services..."

          # Debug the first service to understand the JSON structure
          debug_service "postgres"

          # Use the more reliable inspect method
          check_health_inspect "postgres" 180
          check_health_inspect "redis" 180

          # Wait a bit for Airflow services to stabilize
          sleep 30

          # Check Airflow services
          echo "Checking Airflow services..."
          services=("airflow-apiserver" "airflow-scheduler" "airflow-dag-processor" "airflow-worker" "airflow-triggerer")
          for service in "${services[@]}"; do
            check_health_inspect "$service" 300
          done

          echo "üéâ All services are healthy!"

      - name: Verify container status
        run: |
          echo "=== Container Status ==="
          docker compose ps

          echo -e "\n=== Verifying all expected containers are running ==="
          expected_services=("postgres" "redis" "airflow-apiserver" "airflow-scheduler" "airflow-dag-processor" "airflow-worker" "airflow-triggerer")

          all_healthy=true

          for service in "${expected_services[@]}"; do
            # Simple approach: check if the service appears as "Up" and optionally "healthy" in docker compose ps
            if docker compose ps "$service" --format "table {{.Service}}\t{{.Status}}" | grep -E "(Up|healthy)" > /dev/null; then
              echo "‚úÖ $service is running"
            else
              echo "‚ùå $service is not running properly"
              echo "Status for $service:"
              docker compose ps "$service"
              echo "Recent logs for $service:"
              docker compose logs "$service" --tail=20
              all_healthy=false
            fi
          done

          if [ "$all_healthy" = true ]; then
            echo -e "\nüéâ All expected services are running!"
          else
            echo -e "\n‚ùå Some services are not healthy"
            exit 1
          fi

      - name: Test Airflow Web UI and API
        run: |
          echo "=== Testing Airflow Web UI ==="

          # Test basic connectivity
          echo "Testing basic connectivity to localhost:8080..."
          status_code=$(curl -s -o /dev/null -w "%{http_code}" --connect-timeout 10 http://localhost:8080/ 2>/dev/null)
          echo "Root path returned HTTP status: $status_code"

          if [[ "$status_code" == "200" ]] || [[ "$status_code" == "302" ]]; then
              echo "‚úÖ Airflow web server is accessible"
          else
              echo "‚ùå Airflow web server is not accessible"
              echo "Checking container status..."
              docker compose ps airflow-apiserver
              echo "Checking recent logs..."
              docker compose logs airflow-apiserver --tail=10
              exit 1
          fi

          # Test what's actually available
          echo -e "\n=== Testing available endpoints ==="

          endpoints=(
              "/"
              "/login"
              "/admin/"
              "/api/v1/version"
              "/api/v2/version"
          )

          working_api=""

          for endpoint in "${endpoints[@]}"; do
              status=$(curl -s -o /dev/null -w "%{http_code}" --connect-timeout 5 http://localhost:8080$endpoint 2>/dev/null)
              echo "  $endpoint: HTTP $status"
              
              # Check if this is a working API version endpoint
              if [[ "$endpoint" == "/api/v"* ]] && [[ "$status" == "200" || "$status" == "401" ]]; then
                  working_api=$(echo $endpoint | grep -o "v[0-9]")
                  echo "    ‚Üí Found working API version: $working_api"
              fi
          done

          # If we found a working API, test it with authentication
          if [ -n "$working_api" ]; then
              echo -e "\n=== Testing API with authentication ==="
              echo "Testing API $working_api with default credentials..."
              
              # Test version endpoint
              echo "Checking version..."
              version_response=$(curl -s -u airflow:airflow http://localhost:8080/api/$working_api/version 2>/dev/null)
              version_status=$(curl -s -o /dev/null -w "%{http_code}" -u airflow:airflow http://localhost:8080/api/$working_api/version 2>/dev/null)
              echo "Version endpoint status: $version_status"
              if [ "$version_status" = "200" ]; then
                  echo "Version response: $version_response"
              fi
              
              # Test DAGs endpoint
              echo "Checking DAGs..."
              dags_status=$(curl -s -o /dev/null -w "%{http_code}" -u airflow:airflow http://localhost:8080/api/$working_api/dags 2>/dev/null)
              echo "DAGs endpoint status: $dags_status"
              
              if [ "$dags_status" = "200" ]; then
                  dags_response=$(curl -s -u airflow:airflow http://localhost:8080/api/$working_api/dags 2>/dev/null)
                  if echo "$dags_response" | grep -q "test_dag"; then
                      echo "‚úÖ Test DAG found!"
                  else
                      echo "Available DAGs:"
                      echo "$dags_response" | jq -r '.dags[]?.dag_id // empty' 2>/dev/null | head -5 || echo "Could not parse DAG list"
                  fi
              fi
          else
              echo -e "\n‚ö†Ô∏è  No working API version found. This might be normal if authentication is required for all endpoints."
          fi

          echo -e "\n=== Summary ==="
          echo "Airflow web server is running and accessible on port 8080"
          echo "You can access the web UI at: http://localhost:8080"
          echo "Default credentials are typically: airflow / airflow"

      - name: Test CLI commands
        run: |
          echo "Testing Airflow CLI commands..."

          # Test airflow info command as per documentation
          docker compose run --rm airflow-worker airflow info

          # Test airflow version
          echo -e "\nTesting airflow version..."
          docker compose run --rm airflow-worker airflow version

          # List DAGs via CLI
          echo -e "\nListing DAGs via CLI..."
          docker compose run --rm airflow-worker airflow dags list

      - name: Test DAG execution
        run: |

          echo "=== Testing DAG execution (Airflow 3.x compatible) ==="

          # First, make sure we have working credentials
          echo "1. Finding working credentials..."

          credentials=(
              "airflow:airflow"
              "admin:admin" 
              "admin:airflow"
          )

          working_creds=""
          for cred in "${credentials[@]}"; do
              status=$(curl -s -o /dev/null -w "%{http_code}" -u "$cred" http://localhost:8080/api/v2/version 2>/dev/null)
              if [ "$status" = "200" ]; then
                  working_creds="$cred"
                  echo "‚úÖ Working credentials: $cred"
                  break
              fi
          done

          # If no credentials work, handle existing user properly
          if [ -z "$working_creds" ]; then
              echo "No working credentials found. Checking existing airflow user..."
              
              # First try to update the existing user's password
              echo "Attempting to update existing user password..."
              update_result=$(docker compose exec -T airflow-apiserver airflow users create \
                  --role Admin \
                  --username airflow \
                  --email airflow@example.com \
                  --firstname Airflow \
                  --lastname Admin \
                  --password airflow \
                  --use-random-password false 2>&1 || echo "UPDATE_FAILED")
              
              if echo "$update_result" | grep -q "already exist"; then
                  echo "User exists. Trying to delete and recreate..."
                  
                  # Delete existing user
                  delete_result=$(docker compose exec -T airflow-apiserver airflow users delete --username airflow 2>&1 || echo "DELETE_FAILED")
                  echo "Delete result: $delete_result"
                  
                  # Wait a moment
                  sleep 3
                  
                  # Create new user
                  create_result=$(docker compose exec -T airflow-apiserver airflow users create \
                      --role Admin \
                      --username airflow \
                      --email airflow@example.com \
                      --firstname Airflow \
                      --lastname Admin \
                      --password airflow 2>&1 || echo "CREATE_FAILED")
                  
                  if echo "$create_result" | grep -q "successfully\|created" && ! echo "$create_result" | grep -q "FAILED\|Error"; then
                      echo "‚úÖ User recreated successfully"
                      working_creds="airflow:airflow"
                      sleep 5
                  else
                      echo "‚ùå Failed to recreate user: $create_result"
                      
                      # Try alternative approach - reset password if user exists
                      echo "Trying password reset approach..."
                      reset_result=$(docker compose exec -T airflow-apiserver bash -c "
                          airflow users list | grep airflow && 
                          echo 'User exists, trying alternative creation...' &&
                          airflow users create --role Admin --username testuser --email test@example.com --firstname Test --lastname User --password testpass 2>/dev/null
                      " 2>&1)
                      
                      if echo "$reset_result" | grep -q "successfully\|created"; then
                          echo "‚úÖ Alternative user created"
                          working_creds="testuser:testpass"
                          sleep 5
                      else
                          echo "‚ùå All user creation attempts failed"
                          echo "Let's check what users exist:"
                          docker compose exec -T airflow-apiserver airflow users list || echo "Could not list users"
                          
                          # Try default admin credentials one more time
                          echo "Testing if default admin credentials work..."
                          for test_cred in "admin:admin" "airflow:airflow"; do
                              status=$(curl -s -o /dev/null -w "%{http_code}" -u "$test_cred" http://localhost:8080/api/v2/version 2>/dev/null)
                              if [ "$status" = "200" ]; then
                                  working_creds="$test_cred"
                                  echo "‚úÖ Found working credentials: $test_cred"
                                  break
                              fi
                          done
                          
                          if [ -z "$working_creds" ]; then
                              echo "‚ùå No working credentials found after all attempts"
                              exit 1
                          fi
                      fi
                  fi
              else
                  echo "‚úÖ User created/updated successfully"
                  working_creds="airflow:airflow"
                  sleep 5
              fi
          fi

          # Verify we can access the API
          echo -e "\n2. Verifying API access..."
          api_status=$(curl -s -o /dev/null -w "%{http_code}" -u "$working_creds" http://localhost:8080/api/v2/version 2>/dev/null)
          if [ "$api_status" != "200" ]; then
              echo "‚ùå API access failed with status: $api_status"
              echo "API response:"
              curl -s -u "$working_creds" http://localhost:8080/api/v2/version 2>/dev/null || echo "No response"
              
              # Check if Airflow is actually running
              echo "Checking Airflow container status..."
              docker compose ps
              
              # Check webserver logs
              echo "Recent webserver logs:"
              docker compose logs --tail=20 airflow-apiserver || docker compose logs --tail=20 airflow-webserver || echo "No webserver logs available"
              
              exit 1
          fi
          echo "‚úÖ API access verified"

          # Test DAG endpoint access using v2 API
          echo -e "\n2.5. Testing DAG endpoint access..."
          dag_test_status=$(curl -s -o /dev/null -w "%{http_code}" -u "$working_creds" http://localhost:8080/api/v2/dags 2>/dev/null)
          if [ "$dag_test_status" != "200" ]; then
              echo "‚ùå DAG endpoint access failed with status: $dag_test_status"
              echo "Response:"
              curl -s -u "$working_creds" http://localhost:8080/api/v2/dags 2>/dev/null || echo "No response"
              
              # Check authentication configuration
              echo "Checking Airflow authentication configuration..."
              docker compose exec -T airflow-apiserver airflow config get-value webserver authenticate 2>/dev/null || echo "Could not get auth config"
              docker compose exec -T airflow-apiserver airflow config get-value api auth_backends 2>/dev/null || echo "Could not get API auth config"
              
              exit 1
          else
              echo "‚úÖ DAG endpoint access verified"
          fi

          # Check if DAGs exist
          echo -e "\n3. Checking available DAGs..."
          dags_response=$(curl -s -u "$working_creds" http://localhost:8080/api/v2/dags 2>/dev/null)

          # Debug: Show raw response if there's an issue
          if ! echo "$dags_response" | jq . >/dev/null 2>&1; then
              echo "‚ùå Invalid JSON response from DAGs endpoint:"
              echo "$dags_response"
              exit 1
          fi

          # Count available DAGs
          dag_count=$(echo "$dags_response" | jq -r '.total_entries // 0' 2>/dev/null)
          if [ "$dag_count" = "null" ] || [ -z "$dag_count" ]; then
              dag_count=0
          fi

          echo "Found $dag_count DAG(s)"

          if [ "$dag_count" -eq 0 ]; then
              echo "‚ùå No DAGs found. Creating a test DAG..."
              
              # Create a test DAG file with correct Airflow 3.x syntax
              cat > /tmp/test_dag.py << 'EOF'
          from datetime import datetime, timedelta
          from airflow import DAG
          from airflow.operators.bash import BashOperator
          from airflow.operators.python import PythonOperator

          def print_hello():
              print("Hello from Airflow 3.x!")
              return "Task completed successfully"

          default_args = {
              'owner': 'airflow',
              'depends_on_past': False,
              'start_date': datetime(2024, 1, 1),
              'email_on_failure': False,
              'email_on_retry': False,
              'retries': 1,
              'retry_delay': timedelta(minutes=5),
          }

          # Use 'schedule' instead of deprecated 'schedule_interval'
          dag = DAG(
              'test_dag',
              default_args=default_args,
              description='A simple test DAG for Airflow 3.x',
              schedule=None,  # Manual trigger only
              catchup=False,
              tags=['test', 'airflow3'],
          )

          # Task 1: Print date
          task1 = BashOperator(
              task_id='print_date',
              bash_command='date',
              dag=dag,
          )

          # Task 2: Python task
          task2 = PythonOperator(
              task_id='python_task',
              python_callable=print_hello,
              dag=dag,
          )

          # Task 3: Sleep and echo
          task3 = BashOperator(
              task_id='sleep_and_echo',
              bash_command='sleep 5 && echo "Test DAG completed successfully with Airflow 3.x"',
              dag=dag,
          )

          # Set task dependencies
          task1 >> task2 >> task3
          EOF
              
              # Copy the DAG file to the Airflow DAGs directory
              echo "Copying test DAG to Airflow container..."
              
              # Ensure dags directory exists
              docker compose exec -T airflow-apiserver mkdir -p /opt/airflow/dags
              
              # Copy DAG file
              docker compose cp /tmp/test_dag.py airflow-apiserver:/opt/airflow/dags/
              
              # Also check scheduler container if it's separate
              if docker compose ps | grep -q scheduler; then
                  echo "Copying DAG to scheduler container as well..."
                  docker compose exec -T airflow-scheduler mkdir -p /opt/airflow/dags 2>/dev/null || true
                  docker compose cp /tmp/test_dag.py airflow-scheduler:/opt/airflow/dags/ 2>/dev/null || echo "Scheduler container copy failed or not needed"
              fi
              
              # Force DAG parsing by triggering refresh
              echo "Triggering DAG refresh..."
              docker compose exec -T airflow-apiserver bash -c "
                  # Try different methods to refresh DAGs
                  airflow dags reserialize 2>/dev/null || 
                  airflow dags trigger-dag-refresh 2>/dev/null || 
                  echo 'Manual refresh commands not available'
              "
              
              # Wait for DAG to be parsed - increased wait time
              echo "Waiting for DAG to be parsed (90 seconds)..."
              for i in {1..18}; do  # 18 * 5 = 90 seconds
                  sleep 5
                  echo "Checking DAG availability... (${i}/18)"
                  
                  dags_response=$(curl -s -u "$working_creds" http://localhost:8080/api/v2/dags 2>/dev/null)
                  dag_count=$(echo "$dags_response" | jq -r '.total_entries // 0' 2>/dev/null)
                  
                  if [ "$dag_count" -gt 0 ]; then
                      echo "‚úÖ DAG detected after ${i} checks"
                      break
                  fi
                  
                  # Every 6th check, show some debug info
                  if [ $((i % 6)) -eq 0 ]; then
                      echo "Debug info at check $i:"
                      docker compose exec -T airflow-apiserver ls -la /opt/airflow/dags/ 2>/dev/null || echo "Cannot list DAG directory"
                  fi
              done
              
              # Final check
              dags_response=$(curl -s -u "$working_creds" http://localhost:8080/api/v2/dags 2>/dev/null)
              dag_count=$(echo "$dags_response" | jq -r '.total_entries // 0' 2>/dev/null)
              
              if [ "$dag_count" -gt 0 ]; then
                  echo "‚úÖ Test DAG created successfully"
              else
                  echo "‚ùå Failed to create test DAG. Checking detailed status..."
                  
                  # Check DAG parsing errors using v2 API
                  echo "Checking for DAG import errors..."
                  import_errors=$(curl -s -u "$working_creds" http://localhost:8080/api/v2/importErrors 2>/dev/null)
                  if echo "$import_errors" | jq -e '.import_errors[]?' >/dev/null 2>&1; then
                      echo "DAG import errors found:"
                      echo "$import_errors" | jq -r '.import_errors[] | "  \(.filename): \(.stack_trace)"' 2>/dev/null
                  else
                      echo "No import errors found via API"
                  fi
                  
                  # Check if DAG file exists and is readable
                  echo "Checking DAG file in container..."
                  docker compose exec -T airflow-apiserver bash -c "
                      echo 'DAG directory contents:'
                      ls -la /opt/airflow/dags/
                      echo -e '\nDAG file syntax check:'
                      python -m py_compile /opt/airflow/dags/test_dag.py 2>&1 || echo 'Syntax check failed'
                      echo -e '\nDAG file permissions:'
                      ls -l /opt/airflow/dags/test_dag.py
                  "
                  
                  # Check logs for DAG processing
                  echo "Checking logs for DAG processing..."
                  if docker compose ps | grep -q scheduler; then
                      echo "Scheduler logs:"
                      docker compose logs --tail=30 airflow-scheduler | grep -i "dag\|test_dag\|error\|parsing" || echo "No relevant scheduler logs"
                  else
                      echo "Webserver/All-in-one logs:"
                      docker compose logs --tail=30 airflow-apiserver | grep -i "dag\|test_dag\|error\|parsing" || echo "No relevant logs"
                  fi
                  
                  exit 1
              fi
          fi

          # Get available DAGs
          echo -e "\n4. Available DAGs:"
          available_dags=$(echo "$dags_response" | jq -r '.dags[]?.dag_id // empty' 2>/dev/null)
          if [ -z "$available_dags" ]; then
              echo "‚ùå Could not retrieve DAG list"
              exit 1
          fi

          echo "$available_dags"

          # Choose first available DAG or prefer test_dag
          dag_to_test=""
          if echo "$available_dags" | grep -q "test_dag"; then
              dag_to_test="test_dag"
              echo "Using test_dag for testing"
          else
              dag_to_test=$(echo "$available_dags" | head -1)
              echo "Using first available DAG: $dag_to_test"
          fi

          # Check if the DAG is paused and unpause it if needed
          echo -e "\n5. Checking DAG status..."
          dag_details=$(curl -s -u "$working_creds" "http://localhost:8080/api/v2/dags/$dag_to_test" 2>/dev/null)
          is_paused=$(echo "$dag_details" | jq -r '.is_paused // false' 2>/dev/null)

          if [ "$is_paused" = "true" ]; then
              echo "DAG is paused. Unpausing..."
              unpause_result=$(curl -s -X PATCH \
                  -u "$working_creds" \
                  -H "Content-Type: application/json" \
                  -d '{"is_paused": false}' \
                  "http://localhost:8080/api/v2/dags/$dag_to_test" 2>/dev/null)
              
              if echo "$unpause_result" | grep -q '"is_paused": false'; then
                  echo "‚úÖ DAG unpaused successfully"
              else
                  echo "‚ùå Failed to unpause DAG"
                  echo "Unpause response: $unpause_result"
              fi
          else
              echo "‚úÖ DAG is active"
          fi

          # Trigger the DAG using v2 API
          echo -e "\n6. Triggering DAG: $dag_to_test"
          trigger_response=$(curl -s -X POST \
              -u "$working_creds" \
              -H "Content-Type: application/json" \
              -d '{}' \
              "http://localhost:8080/api/v2/dags/$dag_to_test/dagRuns" 2>/dev/null)

          echo "Trigger response: $trigger_response"

          # Check if the trigger was successful
          dag_run_id=$(echo "$trigger_response" | jq -r '.dag_run_id // empty' 2>/dev/null)
          if [ -n "$dag_run_id" ] && [ "$dag_run_id" != "null" ] && [ "$dag_run_id" != "empty" ]; then
              echo "‚úÖ DAG triggered successfully with run ID: $dag_run_id"
              
              # Wait for execution
              echo -e "\n7. Monitoring DAG execution..."
              for i in {1..30}; do  # Check for up to 150 seconds
                  sleep 5
                  echo "Checking status... (${i}/30)"
                  
                  run_response=$(curl -s -u "$working_creds" \
                      "http://localhost:8080/api/v2/dags/$dag_to_test/dagRuns/$dag_run_id" 2>/dev/null)
                  
                  run_status=$(echo "$run_response" | jq -r '.state // empty' 2>/dev/null)
                  
                  case "$run_status" in
                      "success")
                          echo "üéâ DAG execution completed successfully!"
                          break
                          ;;
                      "failed")
                          echo "‚ùå DAG execution failed"
                          break
                          ;;
                      "running")
                          echo "‚è≥ DAG is still running..."
                          ;;
                      *)
                          echo "‚ÑπÔ∏è DAG status: $run_status"
                          ;;
                  esac
              done
              
              # Final status check
              echo -e "\n8. Final status report..."
              run_response=$(curl -s -u "$working_creds" \
                  "http://localhost:8080/api/v2/dags/$dag_to_test/dagRuns/$dag_run_id" 2>/dev/null)
              
              run_status=$(echo "$run_response" | jq -r '.state // empty' 2>/dev/null)
              start_date=$(echo "$run_response" | jq -r '.start_date // empty' 2>/dev/null)
              end_date=$(echo "$run_response" | jq -r '.end_date // empty' 2>/dev/null)
              
              echo "DAG run status: $run_status"
              echo "Start date: $start_date"
              echo "End date: $end_date"
              
              # Get task instances status using v2 API
              echo -e "\nTask status:"
              tasks_response=$(curl -s -u "$working_creds" \
                  "http://localhost:8080/api/v2/dags/$dag_to_test/dagRuns/$dag_run_id/taskInstances" 2>/dev/null)
              
              if echo "$tasks_response" | jq -e '.task_instances[]?' > /dev/null 2>&1; then
                  echo "$tasks_response" | jq -r '.task_instances[] | "  \(.task_id): \(.state)"' 2>/dev/null
              else
                  echo "  No task information available"
              fi
              
              # If execution failed, show task logs for debugging
              if [ "$run_status" = "failed" ]; then
                  echo -e "\nüîç Checking failed task logs..."
                  failed_tasks=$(echo "$tasks_response" | jq -r '.task_instances[] | select(.state == "failed") | .task_id' 2>/dev/null)
                  
                  if [ -n "$failed_tasks" ]; then
                      echo "Failed tasks: $failed_tasks"
                      for task_id in $failed_tasks; do
                          echo -e "\nLogs for failed task: $task_id"
                          # Note: Log API endpoints may have changed in v2, try different approaches
                          log_response=$(curl -s -u "$working_creds" \
                              "http://localhost:8080/api/v2/dags/$dag_to_test/dagRuns/$dag_run_id/taskInstances/$task_id/logs/1" 2>/dev/null)
                          
                          # Extract log content if available
                          if echo "$log_response" | jq -e '.content?' >/dev/null 2>&1; then
                              echo "$log_response" | jq -r '.content' 2>/dev/null
                          else
                              echo "Log content not available via API, trying container logs..."
                              
                              # Try to get logs from container directly
                              docker compose exec -T airflow-apiserver bash -c "
                                  find /opt/airflow/logs -name '*${task_id}*' -type f | head -5 | xargs tail -20
                              " 2>/dev/null || echo "Could not retrieve logs from container"
                          fi
                      done
                  fi
              fi
              
          else
              echo "‚ùå Failed to trigger DAG"
              echo "Response details: $trigger_response"
              
              # Check for common issues
              if echo "$trigger_response" | grep -q "Not authenticated\|Unauthorized"; then
                  echo "üîç Authentication issue detected"
              elif echo "$trigger_response" | grep -q "DAG.*not found"; then
                  echo "üîç DAG not found issue"
              elif echo "$trigger_response" | grep -q "paused"; then
                  echo "üîç DAG may be paused"
              fi
              exit 1
          fi

          echo -e "\n‚úÖ DAG execution test completed!"

      - name: Clean up
        if: always()
        run: |
          echo "Cleaning up Docker Compose environment..."
          # Stop and remove containers, networks, and volumes as per official documentation
          docker compose down --volumes --remove-orphans

          # Optional: Remove images to free up space
          # docker compose down --volumes --remove-orphans --rmi all

      - name: Show logs on failure
        if: failure()
        run: |
          echo "=== Docker Compose Services Status ==="
          docker compose ps

          echo -e "\n=== System Resources ==="
          df -h
          docker system df

          echo -e "\n=== Airflow Init Logs ==="
          docker compose logs airflow-init || echo "airflow-init container not found"

          echo -e "\n=== Airflow API Server Logs ==="
          docker compose logs airflow-apiserver --tail=50

          echo -e "\n=== Airflow Scheduler Logs ==="
          docker compose logs airflow-scheduler --tail=50

          echo -e "\n=== Airflow DAG Processor Logs ==="
          docker compose logs airflow-dag-processor --tail=50

          echo -e "\n=== Airflow Worker Logs ==="
          docker compose logs airflow-worker --tail=50

          echo -e "\n=== Airflow Triggerer Logs ==="
          docker compose logs airflow-triggerer --tail=50

          echo -e "\n=== Postgres Logs ==="
          docker compose logs postgres --tail=30

          echo -e "\n=== Redis Logs ==="
          docker compose logs redis --tail=30
