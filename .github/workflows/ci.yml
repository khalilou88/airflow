name: Test Airflow Docker Compose

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  workflow_dispatch:

jobs:
  test-airflow-compose:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Check system resources
        run: |
          echo "Checking system resources..."
          echo "Memory available:"
          docker run --rm "debian:bookworm-slim" bash -c 'numfmt --to iec $(echo $(($(getconf _PHYS_PAGES) * $(getconf PAGE_SIZE))))'
          echo "Disk space:"
          df -h
          echo "CPU info:"
          nproc

      - name: Create required directories and environment
        run: |
          # Create directories as per official documentation
          mkdir -p ./dags ./logs ./plugins ./config
          # Set AIRFLOW_UID for Linux as per official docs
          echo "AIRFLOW_UID=$(id -u)" > .env
          # Verify the .env file
          cat .env

      - name: Create a simple test DAG
        run: |
          cat > ./dags/test_dag.py << 'EOF'
          from datetime import datetime, timedelta
          from airflow import DAG
          from airflow.operators.bash import BashOperator

          default_args = {
              'owner': 'test',
              'depends_on_past': False,
              'start_date': datetime(2024, 1, 1),
              'email_on_failure': False,
              'email_on_retry': False,
              'retries': 1,
              'retry_delay': timedelta(minutes=5),
          }

          dag = DAG(
              'test_dag',
              default_args=default_args,
              description='A simple test DAG',
              schedule_interval=timedelta(days=1),
              catchup=False,
              tags=['test'],
          )

          test_task = BashOperator(
              task_id='test_task',
              bash_command='echo "Hello from Airflow test!"',
              dag=dag,
          )
          EOF

      - name: Initialize Airflow database
        run: |
          # Initialize the database as per official documentation
          echo "Running airflow-init to initialize database and create admin user..."
          docker compose up airflow-init

      - name: Start Airflow services
        run: |
          # Start all Airflow services as per official documentation
          echo "Starting Airflow services..."
          docker compose up -d

      - name: Wait for services to be healthy
        run: |
          echo "Waiting for services to be healthy..."

          # Function to check if container is healthy
          check_health() {
            local service=$1
            local timeout=${2:-300}
            local count=0

            while [ $count -lt $timeout ]; do
              # First check if the service exists and is running
              if ! docker compose ps "$service" --format json >/dev/null 2>&1; then
                echo "⚠️  Service $service not found or not running"
                sleep 5
                count=$((count + 5))
                echo "Waiting for $service... ($count/$timeout seconds)"
                continue
              fi

              # Get the JSON output and debug it
              local json_output
              json_output=$(docker compose ps "$service" --format json 2>/dev/null)
              
              if [ -z "$json_output" ]; then
                echo "⚠️  No output from docker compose ps for $service"
                sleep 5
                count=$((count + 5))
                echo "Waiting for $service... ($count/$timeout seconds)"
                continue
              fi

              # Check if it's an array or single object
              local health_status
              if echo "$json_output" | jq -e '. | type' | grep -q "array"; then
                # It's an array, use index 0
                health_status=$(echo "$json_output" | jq -r '.[0].Health // empty' 2>/dev/null)
              else
                # It's a single object
                health_status=$(echo "$json_output" | jq -r '.Health // empty' 2>/dev/null)
              fi

              # If Health field doesn't exist, check State instead
              if [ -z "$health_status" ]; then
                if echo "$json_output" | jq -e '. | type' | grep -q "array"; then
                  health_status=$(echo "$json_output" | jq -r '.[0].State // empty' 2>/dev/null)
                else
                  health_status=$(echo "$json_output" | jq -r '.State // empty' 2>/dev/null)
                fi
              fi

              # Check various healthy states
              if [[ "$health_status" == "healthy" ]] || [[ "$health_status" == "running" ]]; then
                echo "✅ $service is healthy (status: $health_status)"
                return 0
              fi

              sleep 5
              count=$((count + 5))
              echo "Waiting for $service (status: $health_status)... ($count/$timeout seconds)"
            done

            echo "❌ $service failed to become healthy within $timeout seconds"
            return 1
          }

          # Alternative function using docker inspect (more reliable)
          check_health_inspect() {
            local service=$1
            local timeout=${2:-300}
            local count=0

            while [ $count -lt $timeout ]; do
              # Get container ID for the service
              local container_id
              container_id=$(docker compose ps -q "$service" 2>/dev/null)
              
              if [ -z "$container_id" ]; then
                echo "⚠️  Service $service not found"
                sleep 5
                count=$((count + 5))
                echo "Waiting for $service... ($count/$timeout seconds)"
                continue
              fi

              # Check health using docker inspect
              local health_status
              health_status=$(docker inspect "$container_id" --format='{{.State.Health.Status}}' 2>/dev/null)
              
              # If no health check is defined, check if container is running
              if [ "$health_status" = "<no value>" ] || [ -z "$health_status" ]; then
                health_status=$(docker inspect "$container_id" --format='{{.State.Status}}' 2>/dev/null)
              fi

              if [[ "$health_status" == "healthy" ]] || [[ "$health_status" == "running" ]]; then
                echo "✅ $service is healthy (status: $health_status)"
                return 0
              fi

              sleep 5
              count=$((count + 5))
              echo "Waiting for $service (status: $health_status)... ($count/$timeout seconds)"
            done

            echo "❌ $service failed to become healthy within $timeout seconds"
            return 1
          }

          # Function to debug docker compose output
          debug_service() {
            local service=$1
            echo "🔍 Debugging $service:"
            echo "Raw docker compose ps output:"
            docker compose ps "$service" --format json | jq '.' 2>/dev/null || echo "Failed to parse JSON"
            echo "Container status via docker inspect:"
            local container_id
            container_id=$(docker compose ps -q "$service" 2>/dev/null)
            if [ -n "$container_id" ]; then
              docker inspect "$container_id" --format='Health: {{.State.Health.Status}}, Status: {{.State.Status}}' 2>/dev/null
            fi
            echo "---"
          }

          # Wait for core services to be healthy
          echo "Checking core services..."

          # Debug the first service to understand the JSON structure
          debug_service "postgres"

          # Use the more reliable inspect method
          check_health_inspect "postgres" 180
          check_health_inspect "redis" 180

          # Wait a bit for Airflow services to stabilize
          sleep 30

          # Check Airflow services
          echo "Checking Airflow services..."
          services=("airflow-apiserver" "airflow-scheduler" "airflow-dag-processor" "airflow-worker" "airflow-triggerer")
          for service in "${services[@]}"; do
            check_health_inspect "$service" 300
          done

          echo "🎉 All services are healthy!"

      - name: Verify container status
        run: |
          echo "=== Container Status ==="
          docker compose ps

          echo -e "\n=== Verifying all expected containers are running ==="
          expected_services=("postgres" "redis" "airflow-apiserver" "airflow-scheduler" "airflow-dag-processor" "airflow-worker" "airflow-triggerer")

          all_healthy=true

          for service in "${expected_services[@]}"; do
            # Simple approach: check if the service appears as "Up" and optionally "healthy" in docker compose ps
            if docker compose ps "$service" --format "table {{.Service}}\t{{.Status}}" | grep -E "(Up|healthy)" > /dev/null; then
              echo "✅ $service is running"
            else
              echo "❌ $service is not running properly"
              echo "Status for $service:"
              docker compose ps "$service"
              echo "Recent logs for $service:"
              docker compose logs "$service" --tail=20
              all_healthy=false
            fi
          done

          if [ "$all_healthy" = true ]; then
            echo -e "\n🎉 All expected services are running!"
          else
            echo -e "\n❌ Some services are not healthy"
            exit 1
          fi

      - name: Test Airflow Web UI and API
        run: |
          echo "=== Testing Airflow Web UI ==="

          # Test basic connectivity
          echo "Testing basic connectivity to localhost:8080..."
          status_code=$(curl -s -o /dev/null -w "%{http_code}" --connect-timeout 10 http://localhost:8080/ 2>/dev/null)
          echo "Root path returned HTTP status: $status_code"

          if [[ "$status_code" == "200" ]] || [[ "$status_code" == "302" ]]; then
              echo "✅ Airflow web server is accessible"
          else
              echo "❌ Airflow web server is not accessible"
              echo "Checking container status..."
              docker compose ps airflow-apiserver
              echo "Checking recent logs..."
              docker compose logs airflow-apiserver --tail=10
              exit 1
          fi

          # Test what's actually available
          echo -e "\n=== Testing available endpoints ==="

          endpoints=(
              "/"
              "/login"
              "/admin/"
              "/api/v1/version"
              "/api/v2/version"
          )

          working_api=""

          for endpoint in "${endpoints[@]}"; do
              status=$(curl -s -o /dev/null -w "%{http_code}" --connect-timeout 5 http://localhost:8080$endpoint 2>/dev/null)
              echo "  $endpoint: HTTP $status"
              
              # Check if this is a working API version endpoint
              if [[ "$endpoint" == "/api/v"* ]] && [[ "$status" == "200" || "$status" == "401" ]]; then
                  working_api=$(echo $endpoint | grep -o "v[0-9]")
                  echo "    → Found working API version: $working_api"
              fi
          done

          # If we found a working API, test it with authentication
          if [ -n "$working_api" ]; then
              echo -e "\n=== Testing API with authentication ==="
              echo "Testing API $working_api with default credentials..."
              
              # Test version endpoint
              echo "Checking version..."
              version_response=$(curl -s -u airflow:airflow http://localhost:8080/api/$working_api/version 2>/dev/null)
              version_status=$(curl -s -o /dev/null -w "%{http_code}" -u airflow:airflow http://localhost:8080/api/$working_api/version 2>/dev/null)
              echo "Version endpoint status: $version_status"
              if [ "$version_status" = "200" ]; then
                  echo "Version response: $version_response"
              fi
              
              # Test DAGs endpoint
              echo "Checking DAGs..."
              dags_status=$(curl -s -o /dev/null -w "%{http_code}" -u airflow:airflow http://localhost:8080/api/$working_api/dags 2>/dev/null)
              echo "DAGs endpoint status: $dags_status"
              
              if [ "$dags_status" = "200" ]; then
                  dags_response=$(curl -s -u airflow:airflow http://localhost:8080/api/$working_api/dags 2>/dev/null)
                  if echo "$dags_response" | grep -q "test_dag"; then
                      echo "✅ Test DAG found!"
                  else
                      echo "Available DAGs:"
                      echo "$dags_response" | jq -r '.dags[]?.dag_id // empty' 2>/dev/null | head -5 || echo "Could not parse DAG list"
                  fi
              fi
          else
              echo -e "\n⚠️  No working API version found. This might be normal if authentication is required for all endpoints."
          fi

          echo -e "\n=== Summary ==="
          echo "Airflow web server is running and accessible on port 8080"
          echo "You can access the web UI at: http://localhost:8080"
          echo "Default credentials are typically: airflow / airflow"

      - name: Test CLI commands
        run: |
          echo "Testing Airflow CLI commands..."

          # Test airflow info command as per documentation
          docker compose run --rm airflow-worker airflow info

          # Test airflow version
          echo -e "\nTesting airflow version..."
          docker compose run --rm airflow-worker airflow version

          # List DAGs via CLI
          echo -e "\nListing DAGs via CLI..."
          docker compose run --rm airflow-worker airflow dags list






          # Additional debugging step you can add before the DAG execution test
      - name: Debug Airflow setup before DAG test
        run: |



          echo "=== Pre-DAG Test Debugging ==="

          echo "1. Container status:"
          docker compose ps

          echo -e "\n2. Service health status:"
          for service in postgres redis airflow-apiserver airflow-scheduler airflow-worker; do
          if docker compose ps "$service" --format "table {{.Service}}\t{{.Status}}" | grep -q "Up"; then
              echo "  ✅ $service is running"
          else
              echo "  ❌ $service is not running properly"
          fi
          done

          echo -e "\n3. Airflow webserver connectivity:"
          for i in {1..3}; do
          # Try different health endpoints that Airflow commonly uses
          if curl -s -f -m 10 http://localhost:8080/ >/dev/null 2>&1; then
              echo "  ✅ Webserver is responding (attempt $i)"
              break
          elif curl -s -f -m 10 http://localhost:8080/api/v2/version >/dev/null 2>&1; then
              echo "  ✅ Webserver API is responding (attempt $i)"
              break
          else
              echo "  ⚠️  Webserver not responding (attempt $i/3)"
              if [ $i -eq 3 ]; then
              echo "  ❌ Webserver is not accessible"
              echo "  Recent logs:"
              docker compose logs airflow-apiserver --tail=10
              else
              sleep 10
              fi
          fi
          done

          echo -e "\n4. Check if test DAG file exists:"
          if docker compose exec -T airflow-apiserver test -f /opt/airflow/dags/test_dag.py; then
          echo "  ✅ test_dag.py exists"
          echo "  File size: $(docker compose exec -T airflow-apiserver stat -c%s /opt/airflow/dags/test_dag.py) bytes"
          else
          echo "  ❌ test_dag.py not found"
          echo "  Contents of DAGs directory:"
          docker compose exec -T airflow-apiserver ls -la /opt/airflow/dags/
          fi

          echo -e "\n5. Test DAG syntax:"
          docker compose exec -T airflow-apiserver bash -c '
          cd /opt/airflow/dags
          if [ -f test_dag.py ]; then
          echo "Testing DAG syntax..."
          python -m py_compile test_dag.py && echo "✅ Syntax OK" || echo "❌ Syntax error"

          echo "Testing DAG import..."
          python -c "
          import sys
          sys.path.insert(0, \"/opt/airflow/dags\")
          try:
              import test_dag
              print(\"✅ DAG imports successfully\")
              print(f\"DAG ID: {test_dag.dag.dag_id}\")
              print(f\"Task count: {len(test_dag.dag.tasks)}\")
          except Exception as e:
              print(f\"❌ Import error: {e}\")
              import traceback
              traceback.print_exc()
          "
          else
          echo "No test_dag.py found to test"
          fi
          '

          echo -e "\n6. Check Airflow users:"
          docker compose exec -T airflow-apiserver airflow users list || echo "Could not list users"

          echo -e "\n7. Check DAG parsing in Airflow:"
          docker compose exec -T airflow-apiserver airflow dags list | grep test_dag || echo "test_dag not found in Airflow DAG list"

          echo -e "\n8. Check recent Airflow logs for any DAG parsing errors:"
          echo "Scheduler logs:"
          docker compose logs airflow-scheduler --tail=5
          echo -e "\nDAG processor logs:"
          docker compose logs airflow-dag-processor --tail=5

      # Alternative simpler DAG execution test (more focused)
      - name: Simple DAG execution test
        run: |
          echo "=== Simple DAG Execution Test ==="

          # Simple credential test
          echo "Testing API access..."
          if curl -s -f -u "airflow:airflow" "http://localhost:8080/api/v1/version" >/dev/null 2>&1; then
            CREDS="airflow:airflow"
          elif curl -s -f -u "admin:admin" "http://localhost:8080/api/v1/version" >/dev/null 2>&1; then
            CREDS="admin:admin"
          else
            echo "❌ Cannot access Airflow API with default credentials"
            exit 1
          fi
          echo "✅ API accessible with credentials: $CREDS"

          # List available DAGs
          echo -e "\nListing DAGs..."
          DAGS_JSON=$(curl -s -u "$CREDS" "http://localhost:8080/api/v1/dags")
          DAG_COUNT=$(echo "$DAGS_JSON" | jq -r '.total_entries // 0')
          echo "Found $DAG_COUNT DAG(s)"

          if [ "$DAG_COUNT" -eq 0 ]; then
            echo "❌ No DAGs available for testing"
            exit 1
          fi

          # Get first available DAG
          DAG_ID=$(echo "$DAGS_JSON" | jq -r '.dags[0].dag_id')
          echo "Testing with DAG: $DAG_ID"

          # Unpause DAG if needed
          curl -s -X PATCH -u "$CREDS" \
            -H "Content-Type: application/json" \
            -d '{"is_paused": false}' \
            "http://localhost:8080/api/v1/dags/$DAG_ID" >/dev/null

          # Trigger DAG
          echo "Triggering DAG..."
          RUN_ID="test-$(date +%s)"
          TRIGGER_RESP=$(curl -s -X POST -u "$CREDS" \
            -H "Content-Type: application/json" \
            -d "{\"dag_run_id\": \"$RUN_ID\"}" \
            "http://localhost:8080/api/v1/dags/$DAG_ID/dagRuns")

          if echo "$TRIGGER_RESP" | jq -e '.dag_run_id' >/dev/null; then
            echo "✅ DAG triggered successfully"
            
            # Wait a bit and check status
            sleep 30
            STATUS=$(curl -s -u "$CREDS" "http://localhost:8080/api/v1/dags/$DAG_ID/dagRuns/$RUN_ID" | jq -r '.state')
            echo "DAG run status after 30s: $STATUS"
            
            echo "✅ Basic DAG execution test completed"
          else
            echo "❌ Failed to trigger DAG"
            echo "Response: $TRIGGER_RESP"
            exit 1
          fi

      # Environment improvements you can add to your workflow
      - name: Increase Docker resources (if needed)
        run: |
          # For GitHub Actions, we can't increase Docker resources,
          # but we can optimize our containers to use less memory
          echo "Current system resources:"
          free -h
          df -h

      - name: Test DAG execution
        run: |
          echo "=== Testing DAG execution ==="

          # Function to wait with timeout
          wait_with_timeout() {
            local timeout=$1
            local check_command=$2
            local description=$3
            local count=0
            
            echo "Waiting for $description (timeout: ${timeout}s)..."
            while [ $count -lt $timeout ]; do
              if eval "$check_command"; then
                echo "✅ $description completed successfully"
                return 0
              fi
              sleep 5
              count=$((count + 5))
              echo "  Still waiting... ($count/${timeout}s)"
            done
            
            echo "❌ Timeout waiting for $description"
            return 1
          }

          # Step 1: Ensure admin user exists with known credentials
          echo "1. Setting up admin user..."
          docker compose exec -T airflow-apiserver bash -c '
            # Try to create admin user (will fail if exists, that is OK)
            airflow users create \
              --role Admin \
              --username admin \
              --email admin@example.com \
              --firstname Admin \
              --lastname User \
              --password admin 2>/dev/null || echo "User may already exist"
            
            # List users to verify
            echo "Current users:"
            airflow users list
          '

          # Step 2: Verify API access
          echo -e "\n2. Verifying API access..."

          # Test different credential combinations
          CREDS_LIST="admin:admin airflow:airflow"
          WORKING_CREDS=""

          for creds in $CREDS_LIST; do
            echo "Testing credentials: $creds"
            if curl -s -f -u "$creds" "http://localhost:8080/api/v1/version" >/dev/null 2>&1; then
              WORKING_CREDS="$creds"
              echo "✅ Working credentials found: $creds"
              break
            fi
          done

          if [ -z "$WORKING_CREDS" ]; then
            echo "❌ No working credentials found. Trying to diagnose..."
            
            # Check what authentication is configured
            echo "Airflow configuration:"
            docker compose exec -T airflow-apiserver airflow config get-value webserver authenticate 2>/dev/null || echo "Could not get auth config"
            
            # Check if webserver is responding
            echo "Testing webserver response:"
            curl -v http://localhost:8080/health 2>&1 | head -10
            
            echo "Available users:"
            docker compose exec -T airflow-apiserver airflow users list 2>/dev/null || echo "Could not list users"
            
            exit 1
          fi

          # Step 3: Check for existing DAGs
          echo -e "\n3. Checking existing DAGs..."

          DAGS_RESPONSE=$(curl -s -u "$WORKING_CREDS" "http://localhost:8080/api/v1/dags" 2>/dev/null)
          DAG_COUNT=$(echo "$DAGS_RESPONSE" | jq -r '.total_entries // 0' 2>/dev/null)

          echo "Found $DAG_COUNT existing DAG(s)"

          if [ "$DAG_COUNT" -eq 0 ]; then
            echo "No DAGs found. Let's wait a bit for DAG scanning..."
            
            # Wait for DAGs to be scanned (our test DAG should be there)
            wait_with_timeout 60 '
              DAGS_RESPONSE=$(curl -s -u "$WORKING_CREDS" "http://localhost:8080/api/v1/dags" 2>/dev/null)
              DAG_COUNT=$(echo "$DAGS_RESPONSE" | jq -r ".total_entries // 0" 2>/dev/null)
              [ "$DAG_COUNT" -gt 0 ]
            ' "DAG scanning"
            
            # Refresh DAG count
            DAGS_RESPONSE=$(curl -s -u "$WORKING_CREDS" "http://localhost:8080/api/v1/dags" 2>/dev/null)
            DAG_COUNT=$(echo "$DAGS_RESPONSE" | jq -r '.total_entries // 0' 2>/dev/null)
          fi

          if [ "$DAG_COUNT" -eq 0 ]; then
            echo "❌ Still no DAGs found after waiting. Checking DAG directory..."
            docker compose exec -T airflow-apiserver ls -la /opt/airflow/dags/
            
            echo "Checking for DAG parsing errors..."
            docker compose exec -T airflow-apiserver bash -c '
              python -c "
              import sys
              sys.path.append(\"/opt/airflow/dags\")
              try:
                  import test_dag
                  print(\"✅ test_dag imported successfully\")
                  print(f\"DAG ID: {test_dag.dag.dag_id}\")
              except Exception as e:
                  print(f\"❌ Error importing test_dag: {e}\")
              "
            '
            exit 1
          fi

          # Step 4: Get available DAGs and select one for testing
          echo -e "\n4. Selecting DAG for testing..."

          AVAILABLE_DAGS=$(echo "$DAGS_RESPONSE" | jq -r '.dags[].dag_id' 2>/dev/null)
          echo "Available DAGs:"
          echo "$AVAILABLE_DAGS"

          # Prefer test_dag if available, otherwise use first DAG
          if echo "$AVAILABLE_DAGS" | grep -q "test_dag"; then
            DAG_TO_TEST="test_dag"
          else
            DAG_TO_TEST=$(echo "$AVAILABLE_DAGS" | head -1)
          fi

          echo "Selected DAG for testing: $DAG_TO_TEST"

          # Step 5: Ensure DAG is unpaused
          echo -e "\n5. Ensuring DAG is unpaused..."

          # Get current DAG state
          DAG_DETAILS=$(curl -s -u "$WORKING_CREDS" "http://localhost:8080/api/v1/dags/$DAG_TO_TEST" 2>/dev/null)
          IS_PAUSED=$(echo "$DAG_DETAILS" | jq -r '.is_paused // true' 2>/dev/null)

          if [ "$IS_PAUSED" = "true" ]; then
            echo "DAG is paused, unpausing..."
            UNPAUSE_RESULT=$(curl -s -X PATCH \
              -u "$WORKING_CREDS" \
              -H "Content-Type: application/json" \
              -d '{"is_paused": false}' \
              "http://localhost:8080/api/v1/dags/$DAG_TO_TEST" 2>/dev/null)
            
            if echo "$UNPAUSE_RESULT" | jq -e '.is_paused == false' >/dev/null 2>&1; then
              echo "✅ DAG unpaused successfully"
            else
              echo "❌ Failed to unpause DAG: $UNPAUSE_RESULT"
              exit 1
            fi
          else
            echo "✅ DAG is already active"
          fi

          # Step 6: Trigger DAG run
          echo -e "\n6. Triggering DAG run..."

          TRIGGER_RESPONSE=$(curl -s -X POST \
            -u "$WORKING_CREDS" \
            -H "Content-Type: application/json" \
            -d "{\"dag_run_id\": \"test-run-$(date +%s)\"}" \
            "http://localhost:8080/api/v1/dags/$DAG_TO_TEST/dagRuns" 2>/dev/null)

          DAG_RUN_ID=$(echo "$TRIGGER_RESPONSE" | jq -r '.dag_run_id // empty' 2>/dev/null)

          if [ -n "$DAG_RUN_ID" ] && [ "$DAG_RUN_ID" != "null" ] && [ "$DAG_RUN_ID" != "empty" ]; then
            echo "✅ DAG triggered successfully with run ID: $DAG_RUN_ID"
          else
            echo "❌ Failed to trigger DAG"
            echo "Response: $TRIGGER_RESPONSE"
            
            # Additional debugging
            echo "Checking DAG state..."
            curl -s -u "$WORKING_CREDS" "http://localhost:8080/api/v1/dags/$DAG_TO_TEST" | jq '.'
            exit 1
          fi

          # Step 7: Monitor execution (simplified)
          echo -e "\n7. Monitoring DAG execution..."

          wait_with_timeout 120 '
            RUN_RESPONSE=$(curl -s -u "$WORKING_CREDS" "http://localhost:8080/api/v1/dags/$DAG_TO_TEST/dagRuns/$DAG_RUN_ID" 2>/dev/null)
            RUN_STATE=$(echo "$RUN_RESPONSE" | jq -r ".state // \"unknown\"" 2>/dev/null)
            
            echo "  Current state: $RUN_STATE"
            
            [ "$RUN_STATE" = "success" ] || [ "$RUN_STATE" = "failed" ]
          ' "DAG execution completion"

          # Step 8: Final status check
          echo -e "\n8. Final status check..."

          FINAL_RESPONSE=$(curl -s -u "$WORKING_CREDS" "http://localhost:8080/api/v1/dags/$DAG_TO_TEST/dagRuns/$DAG_RUN_ID" 2>/dev/null)
          FINAL_STATE=$(echo "$FINAL_RESPONSE" | jq -r '.state // "unknown"' 2>/dev/null)

          echo "Final DAG run state: $FINAL_STATE"

          # Get task instances
          TASK_INSTANCES=$(curl -s -u "$WORKING_CREDS" "http://localhost:8080/api/v1/dags/$DAG_TO_TEST/dagRuns/$DAG_RUN_ID/taskInstances" 2>/dev/null)

          if echo "$TASK_INSTANCES" | jq -e '.task_instances[]?' >/dev/null 2>&1; then
            echo "Task statuses:"
            echo "$TASK_INSTANCES" | jq -r '.task_instances[] | "  \(.task_id): \(.state)"' 2>/dev/null
          fi

          # Determine success
          case "$FINAL_STATE" in
            "success")
              echo "🎉 DAG execution completed successfully!"
              ;;
            "failed")
              echo "❌ DAG execution failed"
              
              # Show failed task details
              echo "Failed tasks:"
              echo "$TASK_INSTANCES" | jq -r '.task_instances[] | select(.state == "failed") | "  \(.task_id): \(.state)"' 2>/dev/null
              exit 1
              ;;
            *)
              echo "⚠️  DAG execution ended with state: $FINAL_STATE"
              if [ "$FINAL_STATE" != "success" ]; then
                exit 1
              fi
              ;;
          esac

          echo "✅ DAG execution test completed successfully!"

      - name: Test DAG execution
        run: |

          echo "=== Testing DAG execution (Airflow 3.x compatible) ==="

          # First, make sure we have working credentials
          echo "1. Finding working credentials..."

          credentials=(
              "airflow:airflow"
              "admin:admin" 
              "admin:airflow"
          )

          working_creds=""
          for cred in "${credentials[@]}"; do
              status=$(curl -s -o /dev/null -w "%{http_code}" -u "$cred" http://localhost:8080/api/v2/version 2>/dev/null)
              if [ "$status" = "200" ]; then
                  working_creds="$cred"
                  echo "✅ Working credentials: $cred"
                  break
              fi
          done

          # If no credentials work, handle existing user properly
          if [ -z "$working_creds" ]; then
              echo "No working credentials found. Checking existing airflow user..."
              
              # First try to update the existing user's password
              echo "Attempting to update existing user password..."
              update_result=$(docker compose exec -T airflow-apiserver airflow users create \
                  --role Admin \
                  --username airflow \
                  --email airflow@example.com \
                  --firstname Airflow \
                  --lastname Admin \
                  --password airflow \
                  --use-random-password false 2>&1 || echo "UPDATE_FAILED")
              
              if echo "$update_result" | grep -q "already exist"; then
                  echo "User exists. Trying to delete and recreate..."
                  
                  # Delete existing user
                  delete_result=$(docker compose exec -T airflow-apiserver airflow users delete --username airflow 2>&1 || echo "DELETE_FAILED")
                  echo "Delete result: $delete_result"
                  
                  # Wait a moment
                  sleep 3
                  
                  # Create new user
                  create_result=$(docker compose exec -T airflow-apiserver airflow users create \
                      --role Admin \
                      --username airflow \
                      --email airflow@example.com \
                      --firstname Airflow \
                      --lastname Admin \
                      --password airflow 2>&1 || echo "CREATE_FAILED")
                  
                  if echo "$create_result" | grep -q "successfully\|created" && ! echo "$create_result" | grep -q "FAILED\|Error"; then
                      echo "✅ User recreated successfully"
                      working_creds="airflow:airflow"
                      sleep 5
                  else
                      echo "❌ Failed to recreate user: $create_result"
                      
                      # Try alternative approach - reset password if user exists
                      echo "Trying password reset approach..."
                      reset_result=$(docker compose exec -T airflow-apiserver bash -c "
                          airflow users list | grep airflow && 
                          echo 'User exists, trying alternative creation...' &&
                          airflow users create --role Admin --username testuser --email test@example.com --firstname Test --lastname User --password testpass 2>/dev/null
                      " 2>&1)
                      
                      if echo "$reset_result" | grep -q "successfully\|created"; then
                          echo "✅ Alternative user created"
                          working_creds="testuser:testpass"
                          sleep 5
                      else
                          echo "❌ All user creation attempts failed"
                          echo "Let's check what users exist:"
                          docker compose exec -T airflow-apiserver airflow users list || echo "Could not list users"
                          
                          # Try default admin credentials one more time
                          echo "Testing if default admin credentials work..."
                          for test_cred in "admin:admin" "airflow:airflow"; do
                              status=$(curl -s -o /dev/null -w "%{http_code}" -u "$test_cred" http://localhost:8080/api/v2/version 2>/dev/null)
                              if [ "$status" = "200" ]; then
                                  working_creds="$test_cred"
                                  echo "✅ Found working credentials: $test_cred"
                                  break
                              fi
                          done
                          
                          if [ -z "$working_creds" ]; then
                              echo "❌ No working credentials found after all attempts"
                              exit 1
                          fi
                      fi
                  fi
              else
                  echo "✅ User created/updated successfully"
                  working_creds="airflow:airflow"
                  sleep 5
              fi
          fi

          # Verify we can access the API
          echo -e "\n2. Verifying API access..."
          api_status=$(curl -s -o /dev/null -w "%{http_code}" -u "$working_creds" http://localhost:8080/api/v2/version 2>/dev/null)
          if [ "$api_status" != "200" ]; then
              echo "❌ API access failed with status: $api_status"
              echo "API response:"
              curl -s -u "$working_creds" http://localhost:8080/api/v2/version 2>/dev/null || echo "No response"
              
              # Check if Airflow is actually running
              echo "Checking Airflow container status..."
              docker compose ps
              
              # Check webserver logs
              echo "Recent webserver logs:"
              docker compose logs --tail=20 airflow-apiserver || docker compose logs --tail=20 airflow-webserver || echo "No webserver logs available"
              
              exit 1
          fi
          echo "✅ API access verified"

          # Test DAG endpoint access using v2 API
          echo -e "\n2.5. Testing DAG endpoint access..."
          dag_test_status=$(curl -s -o /dev/null -w "%{http_code}" -u "$working_creds" http://localhost:8080/api/v2/dags 2>/dev/null)
          if [ "$dag_test_status" != "200" ]; then
              echo "❌ DAG endpoint access failed with status: $dag_test_status"
              echo "Response:"
              curl -s -u "$working_creds" http://localhost:8080/api/v2/dags 2>/dev/null || echo "No response"
              
              # Check authentication configuration
              echo "Checking Airflow authentication configuration..."
              docker compose exec -T airflow-apiserver airflow config get-value webserver authenticate 2>/dev/null || echo "Could not get auth config"
              docker compose exec -T airflow-apiserver airflow config get-value api auth_backends 2>/dev/null || echo "Could not get API auth config"
              
              exit 1
          else
              echo "✅ DAG endpoint access verified"
          fi

          # Check if DAGs exist
          echo -e "\n3. Checking available DAGs..."
          dags_response=$(curl -s -u "$working_creds" http://localhost:8080/api/v2/dags 2>/dev/null)

          # Debug: Show raw response if there's an issue
          if ! echo "$dags_response" | jq . >/dev/null 2>&1; then
              echo "❌ Invalid JSON response from DAGs endpoint:"
              echo "$dags_response"
              exit 1
          fi

          # Count available DAGs
          dag_count=$(echo "$dags_response" | jq -r '.total_entries // 0' 2>/dev/null)
          if [ "$dag_count" = "null" ] || [ -z "$dag_count" ]; then
              dag_count=0
          fi

          echo "Found $dag_count DAG(s)"

          if [ "$dag_count" -eq 0 ]; then
              echo "❌ No DAGs found. Creating a test DAG..."
              
              # Create a test DAG file with correct Airflow 3.x syntax
              cat > /tmp/test_dag.py << 'EOF'
          from datetime import datetime, timedelta
          from airflow import DAG
          from airflow.operators.bash import BashOperator
          from airflow.operators.python import PythonOperator

          def print_hello():
              print("Hello from Airflow 3.x!")
              return "Task completed successfully"

          default_args = {
              'owner': 'airflow',
              'depends_on_past': False,
              'start_date': datetime(2024, 1, 1),
              'email_on_failure': False,
              'email_on_retry': False,
              'retries': 1,
              'retry_delay': timedelta(minutes=5),
          }

          # Use 'schedule' instead of deprecated 'schedule_interval'
          dag = DAG(
              'test_dag',
              default_args=default_args,
              description='A simple test DAG for Airflow 3.x',
              schedule=None,  # Manual trigger only
              catchup=False,
              tags=['test', 'airflow3'],
          )

          # Task 1: Print date
          task1 = BashOperator(
              task_id='print_date',
              bash_command='date',
              dag=dag,
          )

          # Task 2: Python task
          task2 = PythonOperator(
              task_id='python_task',
              python_callable=print_hello,
              dag=dag,
          )

          # Task 3: Sleep and echo
          task3 = BashOperator(
              task_id='sleep_and_echo',
              bash_command='sleep 5 && echo "Test DAG completed successfully with Airflow 3.x"',
              dag=dag,
          )

          # Set task dependencies
          task1 >> task2 >> task3
          EOF
              
              # Copy the DAG file to the Airflow DAGs directory
              echo "Copying test DAG to Airflow container..."
              
              # Ensure dags directory exists
              docker compose exec -T airflow-apiserver mkdir -p /opt/airflow/dags
              
              # Copy DAG file
              docker compose cp /tmp/test_dag.py airflow-apiserver:/opt/airflow/dags/
              
              # Also check scheduler container if it's separate
              if docker compose ps | grep -q scheduler; then
                  echo "Copying DAG to scheduler container as well..."
                  docker compose exec -T airflow-scheduler mkdir -p /opt/airflow/dags 2>/dev/null || true
                  docker compose cp /tmp/test_dag.py airflow-scheduler:/opt/airflow/dags/ 2>/dev/null || echo "Scheduler container copy failed or not needed"
              fi
              
              # Force DAG parsing by triggering refresh
              echo "Triggering DAG refresh..."
              docker compose exec -T airflow-apiserver bash -c "
                  # Try different methods to refresh DAGs
                  airflow dags reserialize 2>/dev/null || 
                  airflow dags trigger-dag-refresh 2>/dev/null || 
                  echo 'Manual refresh commands not available'
              "
              
              # Wait for DAG to be parsed - increased wait time
              echo "Waiting for DAG to be parsed (90 seconds)..."
              for i in {1..18}; do  # 18 * 5 = 90 seconds
                  sleep 5
                  echo "Checking DAG availability... (${i}/18)"
                  
                  dags_response=$(curl -s -u "$working_creds" http://localhost:8080/api/v2/dags 2>/dev/null)
                  dag_count=$(echo "$dags_response" | jq -r '.total_entries // 0' 2>/dev/null)
                  
                  if [ "$dag_count" -gt 0 ]; then
                      echo "✅ DAG detected after ${i} checks"
                      break
                  fi
                  
                  # Every 6th check, show some debug info
                  if [ $((i % 6)) -eq 0 ]; then
                      echo "Debug info at check $i:"
                      docker compose exec -T airflow-apiserver ls -la /opt/airflow/dags/ 2>/dev/null || echo "Cannot list DAG directory"
                  fi
              done
              
              # Final check
              dags_response=$(curl -s -u "$working_creds" http://localhost:8080/api/v2/dags 2>/dev/null)
              dag_count=$(echo "$dags_response" | jq -r '.total_entries // 0' 2>/dev/null)
              
              if [ "$dag_count" -gt 0 ]; then
                  echo "✅ Test DAG created successfully"
              else
                  echo "❌ Failed to create test DAG. Checking detailed status..."
                  
                  # Check DAG parsing errors using v2 API
                  echo "Checking for DAG import errors..."
                  import_errors=$(curl -s -u "$working_creds" http://localhost:8080/api/v2/importErrors 2>/dev/null)
                  if echo "$import_errors" | jq -e '.import_errors[]?' >/dev/null 2>&1; then
                      echo "DAG import errors found:"
                      echo "$import_errors" | jq -r '.import_errors[] | "  \(.filename): \(.stack_trace)"' 2>/dev/null
                  else
                      echo "No import errors found via API"
                  fi
                  
                  # Check if DAG file exists and is readable
                  echo "Checking DAG file in container..."
                  docker compose exec -T airflow-apiserver bash -c "
                      echo 'DAG directory contents:'
                      ls -la /opt/airflow/dags/
                      echo -e '\nDAG file syntax check:'
                      python -m py_compile /opt/airflow/dags/test_dag.py 2>&1 || echo 'Syntax check failed'
                      echo -e '\nDAG file permissions:'
                      ls -l /opt/airflow/dags/test_dag.py
                  "
                  
                  # Check logs for DAG processing
                  echo "Checking logs for DAG processing..."
                  if docker compose ps | grep -q scheduler; then
                      echo "Scheduler logs:"
                      docker compose logs --tail=30 airflow-scheduler | grep -i "dag\|test_dag\|error\|parsing" || echo "No relevant scheduler logs"
                  else
                      echo "Webserver/All-in-one logs:"
                      docker compose logs --tail=30 airflow-apiserver | grep -i "dag\|test_dag\|error\|parsing" || echo "No relevant logs"
                  fi
                  
                  exit 1
              fi
          fi

          # Get available DAGs
          echo -e "\n4. Available DAGs:"
          available_dags=$(echo "$dags_response" | jq -r '.dags[]?.dag_id // empty' 2>/dev/null)
          if [ -z "$available_dags" ]; then
              echo "❌ Could not retrieve DAG list"
              exit 1
          fi

          echo "$available_dags"

          # Choose first available DAG or prefer test_dag
          dag_to_test=""
          if echo "$available_dags" | grep -q "test_dag"; then
              dag_to_test="test_dag"
              echo "Using test_dag for testing"
          else
              dag_to_test=$(echo "$available_dags" | head -1)
              echo "Using first available DAG: $dag_to_test"
          fi

          # Check if the DAG is paused and unpause it if needed
          echo -e "\n5. Checking DAG status..."
          dag_details=$(curl -s -u "$working_creds" "http://localhost:8080/api/v2/dags/$dag_to_test" 2>/dev/null)
          is_paused=$(echo "$dag_details" | jq -r '.is_paused // false' 2>/dev/null)

          if [ "$is_paused" = "true" ]; then
              echo "DAG is paused. Unpausing..."
              unpause_result=$(curl -s -X PATCH \
                  -u "$working_creds" \
                  -H "Content-Type: application/json" \
                  -d '{"is_paused": false}' \
                  "http://localhost:8080/api/v2/dags/$dag_to_test" 2>/dev/null)
              
              if echo "$unpause_result" | grep -q '"is_paused": false'; then
                  echo "✅ DAG unpaused successfully"
              else
                  echo "❌ Failed to unpause DAG"
                  echo "Unpause response: $unpause_result"
              fi
          else
              echo "✅ DAG is active"
          fi

          # Trigger the DAG using v2 API
          echo -e "\n6. Triggering DAG: $dag_to_test"
          trigger_response=$(curl -s -X POST \
              -u "$working_creds" \
              -H "Content-Type: application/json" \
              -d '{}' \
              "http://localhost:8080/api/v2/dags/$dag_to_test/dagRuns" 2>/dev/null)

          echo "Trigger response: $trigger_response"

          # Check if the trigger was successful
          dag_run_id=$(echo "$trigger_response" | jq -r '.dag_run_id // empty' 2>/dev/null)
          if [ -n "$dag_run_id" ] && [ "$dag_run_id" != "null" ] && [ "$dag_run_id" != "empty" ]; then
              echo "✅ DAG triggered successfully with run ID: $dag_run_id"
              
              # Wait for execution
              echo -e "\n7. Monitoring DAG execution..."
              for i in {1..30}; do  # Check for up to 150 seconds
                  sleep 5
                  echo "Checking status... (${i}/30)"
                  
                  run_response=$(curl -s -u "$working_creds" \
                      "http://localhost:8080/api/v2/dags/$dag_to_test/dagRuns/$dag_run_id" 2>/dev/null)
                  
                  run_status=$(echo "$run_response" | jq -r '.state // empty' 2>/dev/null)
                  
                  case "$run_status" in
                      "success")
                          echo "🎉 DAG execution completed successfully!"
                          break
                          ;;
                      "failed")
                          echo "❌ DAG execution failed"
                          break
                          ;;
                      "running")
                          echo "⏳ DAG is still running..."
                          ;;
                      *)
                          echo "ℹ️ DAG status: $run_status"
                          ;;
                  esac
              done
              
              # Final status check
              echo -e "\n8. Final status report..."
              run_response=$(curl -s -u "$working_creds" \
                  "http://localhost:8080/api/v2/dags/$dag_to_test/dagRuns/$dag_run_id" 2>/dev/null)
              
              run_status=$(echo "$run_response" | jq -r '.state // empty' 2>/dev/null)
              start_date=$(echo "$run_response" | jq -r '.start_date // empty' 2>/dev/null)
              end_date=$(echo "$run_response" | jq -r '.end_date // empty' 2>/dev/null)
              
              echo "DAG run status: $run_status"
              echo "Start date: $start_date"
              echo "End date: $end_date"
              
              # Get task instances status using v2 API
              echo -e "\nTask status:"
              tasks_response=$(curl -s -u "$working_creds" \
                  "http://localhost:8080/api/v2/dags/$dag_to_test/dagRuns/$dag_run_id/taskInstances" 2>/dev/null)
              
              if echo "$tasks_response" | jq -e '.task_instances[]?' > /dev/null 2>&1; then
                  echo "$tasks_response" | jq -r '.task_instances[] | "  \(.task_id): \(.state)"' 2>/dev/null
              else
                  echo "  No task information available"
              fi
              
              # If execution failed, show task logs for debugging
              if [ "$run_status" = "failed" ]; then
                  echo -e "\n🔍 Checking failed task logs..."
                  failed_tasks=$(echo "$tasks_response" | jq -r '.task_instances[] | select(.state == "failed") | .task_id' 2>/dev/null)
                  
                  if [ -n "$failed_tasks" ]; then
                      echo "Failed tasks: $failed_tasks"
                      for task_id in $failed_tasks; do
                          echo -e "\nLogs for failed task: $task_id"
                          # Note: Log API endpoints may have changed in v2, try different approaches
                          log_response=$(curl -s -u "$working_creds" \
                              "http://localhost:8080/api/v2/dags/$dag_to_test/dagRuns/$dag_run_id/taskInstances/$task_id/logs/1" 2>/dev/null)
                          
                          # Extract log content if available
                          if echo "$log_response" | jq -e '.content?' >/dev/null 2>&1; then
                              echo "$log_response" | jq -r '.content' 2>/dev/null
                          else
                              echo "Log content not available via API, trying container logs..."
                              
                              # Try to get logs from container directly
                              docker compose exec -T airflow-apiserver bash -c "
                                  find /opt/airflow/logs -name '*${task_id}*' -type f | head -5 | xargs tail -20
                              " 2>/dev/null || echo "Could not retrieve logs from container"
                          fi
                      done
                  fi
              fi
              
          else
              echo "❌ Failed to trigger DAG"
              echo "Response details: $trigger_response"
              
              # Check for common issues
              if echo "$trigger_response" | grep -q "Not authenticated\|Unauthorized"; then
                  echo "🔍 Authentication issue detected"
              elif echo "$trigger_response" | grep -q "DAG.*not found"; then
                  echo "🔍 DAG not found issue"
              elif echo "$trigger_response" | grep -q "paused"; then
                  echo "🔍 DAG may be paused"
              fi
              exit 1
          fi

          echo -e "\n✅ DAG execution test completed!"

      - name: Clean up
        if: always()
        run: |
          echo "Cleaning up Docker Compose environment..."
          # Stop and remove containers, networks, and volumes as per official documentation
          docker compose down --volumes --remove-orphans

          # Optional: Remove images to free up space
          # docker compose down --volumes --remove-orphans --rmi all

      - name: Show logs on failure
        if: failure()
        run: |
          echo "=== Docker Compose Services Status ==="
          docker compose ps

          echo -e "\n=== System Resources ==="
          df -h
          docker system df

          echo -e "\n=== Airflow Init Logs ==="
          docker compose logs airflow-init || echo "airflow-init container not found"

          echo -e "\n=== Airflow API Server Logs ==="
          docker compose logs airflow-apiserver --tail=50

          echo -e "\n=== Airflow Scheduler Logs ==="
          docker compose logs airflow-scheduler --tail=50

          echo -e "\n=== Airflow DAG Processor Logs ==="
          docker compose logs airflow-dag-processor --tail=50

          echo -e "\n=== Airflow Worker Logs ==="
          docker compose logs airflow-worker --tail=50

          echo -e "\n=== Airflow Triggerer Logs ==="
          docker compose logs airflow-triggerer --tail=50

          echo -e "\n=== Postgres Logs ==="
          docker compose logs postgres --tail=30

          echo -e "\n=== Redis Logs ==="
          docker compose logs redis --tail=30
